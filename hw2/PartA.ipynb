{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PartA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wxyNgQ32EJHx"
      },
      "source": [
        " ## Set up\n",
        " Go to `Edit` -> `Notebook Setting`, make sure that you are using Python3 and GPU. The following command is to check if there is a GPU avaliable. \n",
        "\n",
        " How to use Colab?\n",
        "\n",
        "1.   https://www.geeksforgeeks.org/how-to-use-google-colab/\n",
        "2.  https://colab.research.google.com/notebooks/welcome.ipynb\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "On7xRL-zD6dd",
        "outputId": "36803d95-4f09-47b3-9b94-a4c36c52b414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "# It should print out some info with NVIDIA...\n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Mar  1 01:34:20 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.48.02    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8    16W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VRLetHHV_9nb"
      },
      "source": [
        "# Grading scheme\n",
        "The deadline for HW2 is **March 2nd, 2020**. 10% off for each day. After three days, the assignment will be given a score of zero.\n",
        "\n",
        "You should finish all TODO parts and keep the output cells, and answer the questions. The full credit is 10 points in total. \n",
        "\n",
        "\n",
        "\n",
        "*   Part A: Linear Regression (1 point); PartA2 (1 point). \n",
        "*   Part B: RNN (3 ponts); LSTM (1 point); Question (1 point).\n",
        "*   Part C: Loading dataset (1 point); Model training (1 point); Question (1 point).\n",
        "\n",
        "   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rvFdh-mSEVE9"
      },
      "source": [
        "# PyTorch Basics\n",
        "\n",
        "## Linear regression\n",
        "Let's start with an example. \n",
        "\n",
        "We define a linear function: $y=5x+1.2$, our work now is to see if a simple linear layer can learn the function. \n",
        "We fist generate a number of data points: $(x,y)$, using numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DFKw1GNYEQCO",
        "outputId": "b64174ad-290e-4007-f116-2214c5dfc90e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "\n",
        "number_of_data_point = 50\n",
        "\n",
        "# create sample data for training\n",
        "x_values = np.linspace(0,2,number_of_data_point)\n",
        "x_train = np.array(x_values, dtype=np.float32)\n",
        "x_train = x_train.reshape(-1, 1)\n",
        "\n",
        "# generate sample y data, \n",
        "# y_values = [math.sin(x)+ 0.2 for x in x_values]\n",
        "y_values = [5*x+ 1.2 for x in x_values]\n",
        "y_train = np.array(y_values, dtype=np.float32)\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "\n",
        "# print out the shape\n",
        "print (x_train.shape)\n",
        "# print (x_train)\n",
        "# print (y_train)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YaQm36DKEqXn"
      },
      "source": [
        "### Now start using Pytorch!\n",
        "Now let's create the first \"neural network\" class. \n",
        "\n",
        "We create a `linearRegression` class, this is a basic Pytorch Module.\n",
        "A Pytorch Module is supposed to have two funcitons: `__init___()`: we put all components there, like all the layers; `forward()`: we define the calculation flow in this method.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S_Dns7efEnWp",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "class linearRegression(torch.nn.Module):\n",
        "    def __init__(self, inputSize, outputSize):\n",
        "        super(linearRegression, self).__init__()\n",
        "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "msF19cisE9rw"
      },
      "source": [
        "Define inputs and outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "luCvIzM_E6cI",
        "colab": {}
      },
      "source": [
        "# our input and output are just a scalar, so the dimension should be 1 for both.\n",
        "inputDim = 1   \n",
        "outputDim = 1   \n",
        "learningRate = 0.01 \n",
        "epochs = 100\n",
        "\n",
        "# our model instance\n",
        "model = linearRegression(inputDim, outputDim)\n",
        "\n",
        "\n",
        "##### For GPU #######\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cOA9kNdIFHMH"
      },
      "source": [
        "\n",
        "## Loss function and optimizer\n",
        "\n",
        "Now it is time to define a loss function and an optimizer.  Think about what loss functions to use? And which optimizer? Fill in the code.\n",
        "\n",
        "References:\n",
        "\n",
        "https://pytorch.org/docs/stable/optim.html\n",
        "https://pytorch.org/docs/stable/nn.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NBV9bC0kFOc5",
        "colab": {}
      },
      "source": [
        "## TODO: fill in the loss function and optimizer\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "criterion = nn.MSELoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=learningRate)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=learningRate)\n",
        "optimizer = optim.SGD(model.parameters(), lr=learningRate, momentum=0.9)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WZz60Nk4FXK1"
      },
      "source": [
        "Start training now..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qKcorCJwFSOy",
        "outputId": "32a465a3-0d3e-4cdc-d851-47df547a3646",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    # Converting inputs and labels to Variable\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
        "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
        "    else:\n",
        "        inputs = Variable(torch.from_numpy(x_train))\n",
        "        labels = Variable(torch.from_numpy(y_train))\n",
        "\n",
        "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # get output from the model, given the inputs\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # get loss for the predicted output\n",
        "    loss = criterion(outputs, labels)\n",
        "    print(loss)\n",
        "    # get gradients w.r.t to parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(45.0235, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 0, loss 45.02349853515625\n",
            "tensor(41.2534, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 1, loss 41.25338363647461\n",
            "tensor(34.7014, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 2, loss 34.70137405395508\n",
            "tensor(26.6590, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 3, loss 26.659048080444336\n",
            "tensor(18.4480, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 4, loss 18.448022842407227\n",
            "tensor(11.1939, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 5, loss 11.193936347961426\n",
            "tensor(5.6759, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 6, loss 5.675943374633789\n",
            "tensor(2.2620, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 7, loss 2.262006998062134\n",
            "tensor(0.9244, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 8, loss 0.9243513345718384\n",
            "tensor(1.3173, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 9, loss 1.3172560930252075\n",
            "tensor(2.8925, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 10, loss 2.892535448074341\n",
            "tensor(5.0265, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 11, loss 5.026544570922852\n",
            "tensor(7.1356, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 12, loss 7.135609149932861\n",
            "tensor(8.7631, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 13, loss 8.763111114501953\n",
            "tensor(9.6292, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 14, loss 9.62923812866211\n",
            "tensor(9.6424, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 15, loss 9.64237117767334\n",
            "tensor(8.8774, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 16, loss 8.877420425415039\n",
            "tensor(7.5310, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 17, loss 7.531042098999023\n",
            "tensor(5.8654, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 18, loss 5.86539363861084\n",
            "tensor(4.1519, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 19, loss 4.151899814605713\n",
            "tensor(2.6242, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 20, loss 2.624241352081299\n",
            "tensor(1.4465, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 21, loss 1.4465464353561401\n",
            "tensor(0.6991, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 22, loss 0.6991111636161804\n",
            "tensor(0.3807, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 23, loss 0.38066133856773376\n",
            "tensor(0.4236, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 24, loss 0.4236450493335724\n",
            "tensor(0.7176, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 25, loss 0.7175538539886475\n",
            "tensor(1.1349, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 26, loss 1.1349096298217773\n",
            "tensor(1.5551, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 27, loss 1.5551384687423706\n",
            "tensor(1.8828, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 28, loss 1.8827991485595703\n",
            "tensor(2.0582, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 29, loss 2.058248996734619\n",
            "tensor(2.0604, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 30, loss 2.060413360595703\n",
            "tensor(1.9027, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 31, loss 1.9027189016342163\n",
            "tensor(1.6241, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 32, loss 1.6241389513015747\n",
            "tensor(1.2778, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 33, loss 1.2777633666992188\n",
            "tensor(0.9192, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 34, loss 0.9192333817481995\n",
            "tensor(0.5970, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 35, loss 0.5969685912132263\n",
            "tensor(0.3455, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 36, loss 0.34545257687568665\n",
            "tensor(0.1821, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 37, loss 0.18209411203861237\n",
            "tensor(0.1075, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 38, loss 0.10750669240951538\n",
            "tensor(0.1085, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 39, loss 0.10850383341312408\n",
            "tensor(0.1628, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 40, loss 0.1628025770187378\n",
            "tensor(0.2443, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 41, loss 0.2443375289440155\n",
            "tensor(0.3282, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 42, loss 0.32819411158561707\n",
            "tensor(0.3944, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 43, loss 0.3944217562675476\n",
            "tensor(0.4303, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 44, loss 0.43031278252601624\n",
            "tensor(0.4311, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 45, loss 0.43106338381767273\n",
            "tensor(0.3990, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 46, loss 0.3990125060081482\n",
            "tensor(0.3419, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 47, loss 0.3418585956096649\n",
            "tensor(0.2703, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 48, loss 0.27033549547195435\n",
            "tensor(0.1958, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 49, loss 0.19583556056022644\n",
            "tensor(0.1284, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 50, loss 0.12837988138198853\n",
            "tensor(0.0752, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 51, loss 0.07520141452550888\n",
            "tensor(0.0401, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 52, loss 0.040060266852378845\n",
            "tensor(0.0233, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 53, loss 0.023260358721017838\n",
            "tensor(0.0222, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 54, loss 0.022235188633203506\n",
            "tensor(0.0325, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 55, loss 0.032495252788066864\n",
            "tensor(0.0487, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 56, loss 0.04871385544538498\n",
            "tensor(0.0657, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 57, loss 0.06574588268995285\n",
            "tensor(0.0794, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 58, loss 0.07942432165145874\n",
            "tensor(0.0870, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 59, loss 0.08704691380262375\n",
            "tensor(0.0875, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 60, loss 0.08752969652414322\n",
            "tensor(0.0813, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 61, loss 0.08126675337553024\n",
            "tensor(0.0698, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 62, loss 0.06977596133947372\n",
            "tensor(0.0552, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 63, loss 0.05522601306438446\n",
            "tensor(0.0399, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 64, loss 0.0399487279355526\n",
            "tensor(0.0260, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 65, loss 0.02601795084774494\n",
            "tensor(0.0150, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 66, loss 0.014950252138078213\n",
            "tensor(0.0076, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 67, loss 0.007556272204965353\n",
            "tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 68, loss 0.003936143592000008\n",
            "tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 69, loss 0.003593272529542446\n",
            "tensor(0.0056, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 70, loss 0.005625065416097641\n",
            "tensor(0.0089, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 71, loss 0.00894451979547739\n",
            "tensor(0.0125, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 72, loss 0.012490598484873772\n",
            "tensor(0.0154, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 73, loss 0.015394489280879498\n",
            "tensor(0.0171, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 74, loss 0.017083067446947098\n",
            "tensor(0.0173, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 75, loss 0.017314566299319267\n",
            "tensor(0.0162, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 76, loss 0.016152823343873024\n",
            "tensor(0.0139, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 77, loss 0.013896717689931393\n",
            "tensor(0.0110, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 78, loss 0.010984780266880989\n",
            "tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 79, loss 0.007894479669630527\n",
            "tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 80, loss 0.005055072717368603\n",
            "tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 81, loss 0.0027848484460264444\n",
            "tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 82, loss 0.0012585282092913985\n",
            "tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 83, loss 0.0005048222956247628\n",
            "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 84, loss 0.00042845733696594834\n",
            "tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 85, loss 0.0008484742138534784\n",
            "tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 86, loss 0.0015435069799423218\n",
            "tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 87, loss 0.00229517905972898\n",
            "tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 88, loss 0.002922705840319395\n",
            "tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 89, loss 0.0033050011843442917\n",
            "tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 90, loss 0.0033886136952787638\n",
            "tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 91, loss 0.0031833460088819265\n",
            "tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 92, loss 0.0027480628341436386\n",
            "tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 93, loss 0.0021714370232075453\n",
            "tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 94, loss 0.0015513238031417131\n",
            "tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 95, loss 0.0009766612201929092\n",
            "tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 96, loss 0.0005142423906363547\n",
            "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 97, loss 0.00020172694348730147\n",
            "tensor(4.6786e-05, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 98, loss 4.678631012211554e-05\n",
            "tensor(3.1316e-05, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 99, loss 3.13156888296362e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ii14nmnmFiNf"
      },
      "source": [
        "Print..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ds2XdCD7Fjim",
        "outputId": "1bdb0cb7-57c6-4cef-954a-f9d33b60abc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "with torch.no_grad(): # we don't need gradients in the testing phase\n",
        "    if torch.cuda.is_available():\n",
        "        predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()\n",
        "    else:\n",
        "        predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
        "    # print(predicted)\n",
        "\n",
        "plt.clf()\n",
        "plt.plot(x_train, y_train, 'go', label='True', alpha=0.5)\n",
        "plt.plot(x_train, predicted, 'ro', label='Predictions', alpha=0.5)\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFb2P1wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JW0A6MeBrN6t"
      },
      "source": [
        "## Improving the model\n",
        "\n",
        "Now it is your time to improve the model: how can the prediction be better adapted to the true values? Does two linears help? Try it out!\n",
        "(Add ref)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "163EqEL-IgsX",
        "colab": {}
      },
      "source": [
        "## TODO: make your regression class\n",
        "class myRegression(torch.nn.Module):\n",
        "    def __init__(self,inputSize, outputSize):\n",
        "        super(myRegression, self).__init__()\n",
        "        #...\n",
        "        self.linear1 = torch.nn.Linear(inputSize, inputSize)\n",
        "        self.linear2 = torch.nn.Linear(inputSize, outputSize)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #...\n",
        "        out1 = self.linear1(x)\n",
        "        out = self.linear2(out1)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TAjft2riJjbP",
        "colab": {}
      },
      "source": [
        "# apply your regression class\n",
        "model = myRegression(inputDim, outputDim)\n",
        "criterion = nn.MSELoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=learningRate)\n",
        "optimizer = optim.SGD(model.parameters(), lr=learningRate, momentum=0.9)\n",
        "\n",
        "##### For GPU #######\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GQ_f-RGFJoAv",
        "outputId": "ae9e6a2e-ea90-4a9d-f2d0-066ac4b7c600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# start training\n",
        "for epoch in range(epochs):\n",
        "    # Converting inputs and labels to Variable\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
        "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
        "    else:\n",
        "        inputs = Variable(torch.from_numpy(x_train))\n",
        "        labels = Variable(torch.from_numpy(y_train))\n",
        "\n",
        "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # get output from the model, given the inputs\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # get loss for the predicted output\n",
        "    loss = criterion(outputs, labels)\n",
        "    print(loss)\n",
        "    # get gradients w.r.t to parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(60.0251, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 0, loss 60.02509689331055\n",
            "tensor(57.0901, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 1, loss 57.09014892578125\n",
            "tensor(51.9598, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 2, loss 51.95983123779297\n",
            "tensor(45.0429, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 3, loss 45.04291915893555\n",
            "tensor(35.9589, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 4, loss 35.958919525146484\n",
            "tensor(23.9849, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 5, loss 23.984949111938477\n",
            "tensor(10.2066, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 6, loss 10.20655632019043\n",
            "tensor(1.8700, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 7, loss 1.8700045347213745\n",
            "tensor(10.0554, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 8, loss 10.055380821228027\n",
            "tensor(23.6452, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 9, loss 23.645198822021484\n",
            "tensor(16.7780, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 10, loss 16.778043746948242\n",
            "tensor(4.5259, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 11, loss 4.525904655456543\n",
            "tensor(3.2623, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 12, loss 3.262312889099121\n",
            "tensor(7.4893, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 13, loss 7.489299774169922\n",
            "tensor(10.9698, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 14, loss 10.969758033752441\n",
            "tensor(12.0572, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 15, loss 12.057208061218262\n",
            "tensor(11.2110, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 16, loss 11.21102237701416\n",
            "tensor(9.2987, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 17, loss 9.29874038696289\n",
            "tensor(6.9966, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 18, loss 6.99660587310791\n",
            "tensor(4.7541, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 19, loss 4.754130840301514\n",
            "tensor(2.9732, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 20, loss 2.9731647968292236\n",
            "tensor(2.0783, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 21, loss 2.0782697200775146\n",
            "tensor(2.2107, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 22, loss 2.210742712020874\n",
            "tensor(2.7377, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 23, loss 2.7376697063446045\n",
            "tensor(2.5365, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 24, loss 2.5364880561828613\n",
            "tensor(1.3306, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 25, loss 1.3306186199188232\n",
            "tensor(0.2141, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 26, loss 0.21406522393226624\n",
            "tensor(0.2935, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 27, loss 0.2935273349285126\n",
            "tensor(1.2725, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 28, loss 1.2724987268447876\n",
            "tensor(1.7749, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 29, loss 1.7749351263046265\n",
            "tensor(1.1338, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 30, loss 1.133782982826233\n",
            "tensor(0.3673, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 31, loss 0.36733096837997437\n",
            "tensor(0.6590, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 32, loss 0.659001350402832\n",
            "tensor(1.4582, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 33, loss 1.4581868648529053\n",
            "tensor(1.4152, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 34, loss 1.4151664972305298\n",
            "tensor(0.6538, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 35, loss 0.6538430452346802\n",
            "tensor(0.3144, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 36, loss 0.31439313292503357\n",
            "tensor(0.5997, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 37, loss 0.599664568901062\n",
            "tensor(0.7757, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 38, loss 0.7757064700126648\n",
            "tensor(0.4728, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 39, loss 0.47283247113227844\n",
            "tensor(0.0787, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 40, loss 0.07874379307031631\n",
            "tensor(0.0319, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 41, loss 0.03186379745602608\n",
            "tensor(0.2591, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 42, loss 0.25912147760391235\n",
            "tensor(0.3952, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 43, loss 0.3952081501483917\n",
            "tensor(0.2947, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 44, loss 0.29473310708999634\n",
            "tensor(0.1384, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 45, loss 0.13836202025413513\n",
            "tensor(0.1252, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 46, loss 0.1251707375049591\n",
            "tensor(0.2401, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 47, loss 0.24007350206375122\n",
            "tensor(0.3258, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 48, loss 0.3257690370082855\n",
            "tensor(0.2835, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 49, loss 0.28350892663002014\n",
            "tensor(0.1644, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 50, loss 0.1643514335155487\n",
            "tensor(0.0877, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 51, loss 0.0876905769109726\n",
            "tensor(0.1025, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 52, loss 0.10249696671962738\n",
            "tensor(0.1453, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 53, loss 0.14529363811016083\n",
            "tensor(0.1313, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 54, loss 0.13125869631767273\n",
            "tensor(0.0599, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 55, loss 0.059896036982536316\n",
            "tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 56, loss 0.005107422824949026\n",
            "tensor(0.0152, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 57, loss 0.015200530178844929\n",
            "tensor(0.0584, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 58, loss 0.05844658240675926\n",
            "tensor(0.0735, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 59, loss 0.07348348200321198\n",
            "tensor(0.0470, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 60, loss 0.04704919829964638\n",
            "tensor(0.0206, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 61, loss 0.020623216405510902\n",
            "tensor(0.0286, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 62, loss 0.028619548305869102\n",
            "tensor(0.0553, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 63, loss 0.0552925206720829\n",
            "tensor(0.0623, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 64, loss 0.06232370063662529\n",
            "tensor(0.0410, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 65, loss 0.04098677635192871\n",
            "tensor(0.0177, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 66, loss 0.017695846036076546\n",
            "tensor(0.0147, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 67, loss 0.01471574418246746\n",
            "tensor(0.0247, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 68, loss 0.02470567636191845\n",
            "tensor(0.0269, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 69, loss 0.026851706206798553\n",
            "tensor(0.0151, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 70, loss 0.015094521455466747\n",
            "tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 71, loss 0.002292506629601121\n",
            "tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 72, loss 0.0011862332466989756\n",
            "tensor(0.0095, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 73, loss 0.009463046677410603\n",
            "tensor(0.0153, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 74, loss 0.015284880064427853\n",
            "tensor(0.0126, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 75, loss 0.01260099746286869\n",
            "tensor(0.0066, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 76, loss 0.006577191408723593\n",
            "tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 77, loss 0.005058406386524439\n",
            "tensor(0.0089, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 78, loss 0.008877089247107506\n",
            "tensor(0.0122, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 79, loss 0.012248901650309563\n",
            "tensor(0.0106, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 80, loss 0.010639416985213757\n",
            "tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 81, loss 0.00573194632306695\n",
            "tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 82, loss 0.0024767036084085703\n",
            "tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 83, loss 0.002984657883644104\n",
            "tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 84, loss 0.0047369650565087795\n",
            "tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 85, loss 0.004419910255819559\n",
            "tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 86, loss 0.001982757356017828\n",
            "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 87, loss 0.00012920268636662513\n",
            "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 88, loss 0.0006323425914160907\n",
            "tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 89, loss 0.002376519376412034\n",
            "tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 90, loss 0.003123711096122861\n",
            "tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 91, loss 0.002266450086608529\n",
            "tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 92, loss 0.0011742673814296722\n",
            "tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 93, loss 0.0011762004578486085\n",
            "tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 94, loss 0.0019839522428810596\n",
            "tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 95, loss 0.002332500647753477\n",
            "tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 96, loss 0.0016697683604434133\n",
            "tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 97, loss 0.0006962613551877439\n",
            "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 98, loss 0.0003320591349620372\n",
            "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 99, loss 0.0006272256723605096\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ROWeKmTBJwsP",
        "outputId": "0845c4b8-9603-4be5-d129-f96f13e7773f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "with torch.no_grad(): # we don't need gradients in the testing phase\n",
        "    if torch.cuda.is_available():\n",
        "        predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()\n",
        "    else:\n",
        "        predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
        "    print(predicted)\n",
        "\n",
        "plt.clf()\n",
        "plt.plot(x_train, y_train, 'go', label='True data', alpha=0.5)\n",
        "plt.plot(x_train, predicted, 'ro', label='Predictions', alpha=0.5)\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.1738025]\n",
            " [ 1.3777344]\n",
            " [ 1.5816662]\n",
            " [ 1.7855984]\n",
            " [ 1.9895302]\n",
            " [ 2.1934621]\n",
            " [ 2.3973942]\n",
            " [ 2.601326 ]\n",
            " [ 2.8052578]\n",
            " [ 3.0091896]\n",
            " [ 3.2131217]\n",
            " [ 3.4170535]\n",
            " [ 3.6209855]\n",
            " [ 3.8249173]\n",
            " [ 4.0288496]\n",
            " [ 4.2327814]\n",
            " [ 4.436713 ]\n",
            " [ 4.640645 ]\n",
            " [ 4.844577 ]\n",
            " [ 5.048509 ]\n",
            " [ 5.252441 ]\n",
            " [ 5.4563727]\n",
            " [ 5.6603045]\n",
            " [ 5.864237 ]\n",
            " [ 6.0681686]\n",
            " [ 6.2721004]\n",
            " [ 6.4760323]\n",
            " [ 6.679964 ]\n",
            " [ 6.8838964]\n",
            " [ 7.087828 ]\n",
            " [ 7.29176  ]\n",
            " [ 7.495692 ]\n",
            " [ 7.6996236]\n",
            " [ 7.9035554]\n",
            " [ 8.107488 ]\n",
            " [ 8.3114195]\n",
            " [ 8.515351 ]\n",
            " [ 8.719283 ]\n",
            " [ 8.923215 ]\n",
            " [ 9.127147 ]\n",
            " [ 9.3310795]\n",
            " [ 9.535011 ]\n",
            " [ 9.738943 ]\n",
            " [ 9.942875 ]\n",
            " [10.146807 ]\n",
            " [10.350739 ]\n",
            " [10.554671 ]\n",
            " [10.758603 ]\n",
            " [10.962535 ]\n",
            " [11.166467 ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ea7XpQP0XROO"
      },
      "source": [
        "# Part A2\n",
        "Once you have sshed into the Zoo cluster, copy the homework files to your hidden directory under Homework2 folder.\n",
        "`cp -r /home/classes/cs477/assignments/2020-Homework2 ~/hidden/<YOUR_PIN>`\n",
        "\n",
        "Apply the virtualenv using:\n",
        "`source /home/classes/cs477/venvs/hw2/bin/activate`\n",
        "\n",
        "In this part you will fill the code in `PartA2.py`.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W4NhpKwfrEEZ"
      },
      "source": [
        "## Naïve Bayes\n",
        "Time estimate to run correct implementation: 5 minutes\n",
        "In this part of the assignment, you’ll be training a Naïve Bayes Classifier on the IMDb Movie Review Database. You’ll be using some of the features that come built into the sklearn library to quickly get a Multinomial Naïve Bayes Classifier running. \n",
        "\n",
        "1. To process the movie reviews, we’ll want to convert them into a matrix of word counts for each possible word. We recommend that you check out the CountVectorizer class in sklearn. You can use the nltk word tokenizer as well. Your code for this function should be very brief!\n",
        "2. We’ll train and test a Naïve Bayes Classifier on our dataset. Using sklearn’s MultinomialNB() class, fit a classifier to the reviews and labels of the training set (x_train and y_train). Then return a set of predictions for the test set of reviews (x_test)\n",
        "\n",
        "The result we got with a correct implementation was around:\n",
        "Accuracy: 0.660000\n",
        "(Around this number, may be different). \n",
        "\n",
        "Note that it may take a few minutes to run the script. \n",
        "\n",
        "\n",
        "Fill the code in `PartA2.py`.  \n",
        "\n",
        "*Reference*: https://scikit-learn.org/stable/datasets/index.html#datasets\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OOm0KSXarkqu"
      },
      "source": [
        "### **Submission**\n",
        "You have to submit the code (.ipynb) file, and answer the questions in a Text cell (click `+Text` button to do so), make sure that your answer is in the LAST Text cell. Also make sure that your .ipynb contains all the outputs, i.e., the plots.  You can also save your answer in a `README.txt` file and submit. \n",
        "\n",
        "\n",
        "**Questions:**\n",
        "1. What improvements did you try in the `myRegression` class?  Describe the structure. And do you think it improved? \n",
        "2. Do you think only using `nn.Linear` layers can learn this function: $y=sin(x)$? Why or why not?\n",
        "\n",
        "\n",
        "## Submission\n",
        "\n",
        "Now that you have completed the assignment, follow the steps below to submit your aissgnment:\n",
        "1. Click __Runtime__  > __Run all__ to generate the output for all cells in the notebook.\n",
        "2. Save the notebook ( __File__ >  __Save__) with the output from all the cells in the notebook by click __File__ > __Download .ipynb__.\n",
        "3. Put the .ipynb file and `README.txt` (if you have one) under your hidden directory on the Zoo server `~/hidden/<YOUR_PIN>/Homework2/`.\n",
        "5. As a final step, run a script that will set up the permissions to your homework files, so we can access and run your code to grade it. Make sure the command be;pw runs without errors, and do not make any changes or run the code again. If you do run the code again or make any changes, you need to run the permissions script again. Submissions without the correct permissions may incur some grading penalty.\n",
        "`/home/classes/cs477/bash_files/hw2_set_permissions.sh <YOUR_PIN>`. **Note** that your `PartA2.py` should also be included in your folder."
      ]
    }
  ]
}