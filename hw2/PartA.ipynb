{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PartA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wxyNgQ32EJHx"
      },
      "source": [
        " ## Set up\n",
        " Go to `Edit` -> `Notebook Setting`, make sure that you are using Python3 and GPU. The following command is to check if there is a GPU avaliable. \n",
        "\n",
        " How to use Colab?\n",
        "\n",
        "1.   https://www.geeksforgeeks.org/how-to-use-google-colab/\n",
        "2.  https://colab.research.google.com/notebooks/welcome.ipynb\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "On7xRL-zD6dd",
        "outputId": "36803d95-4f09-47b3-9b94-a4c36c52b414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "# It should print out some info with NVIDIA...\n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Mar  1 01:34:20 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.48.02    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8    16W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VRLetHHV_9nb"
      },
      "source": [
        "# Grading scheme\n",
        "The deadline for HW2 is **March 2nd, 2020**. 10% off for each day. After three days, the assignment will be given a score of zero.\n",
        "\n",
        "You should finish all TODO parts and keep the output cells, and answer the questions. The full credit is 10 points in total. \n",
        "\n",
        "\n",
        "\n",
        "*   Part A: Linear Regression (1 point); PartA2 (1 point). \n",
        "*   Part B: RNN (3 ponts); LSTM (1 point); Question (1 point).\n",
        "*   Part C: Loading dataset (1 point); Model training (1 point); Question (1 point).\n",
        "\n",
        "   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rvFdh-mSEVE9"
      },
      "source": [
        "# PyTorch Basics\n",
        "\n",
        "## Linear regression\n",
        "Let's start with an example. \n",
        "\n",
        "We define a linear function: $y=5x+1.2$, our work now is to see if a simple linear layer can learn the function. \n",
        "We fist generate a number of data points: $(x,y)$, using numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DFKw1GNYEQCO",
        "outputId": "b64174ad-290e-4007-f116-2214c5dfc90e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "\n",
        "number_of_data_point = 50\n",
        "\n",
        "# create sample data for training\n",
        "x_values = np.linspace(0,2,number_of_data_point)\n",
        "x_train = np.array(x_values, dtype=np.float32)\n",
        "x_train = x_train.reshape(-1, 1)\n",
        "\n",
        "# generate sample y data, \n",
        "# y_values = [math.sin(x)+ 0.2 for x in x_values]\n",
        "y_values = [5*x+ 1.2 for x in x_values]\n",
        "y_train = np.array(y_values, dtype=np.float32)\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "\n",
        "# print out the shape\n",
        "print (x_train.shape)\n",
        "# print (x_train)\n",
        "# print (y_train)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YaQm36DKEqXn"
      },
      "source": [
        "### Now start using Pytorch!\n",
        "Now let's create the first \"neural network\" class. \n",
        "\n",
        "We create a `linearRegression` class, this is a basic Pytorch Module.\n",
        "A Pytorch Module is supposed to have two funcitons: `__init___()`: we put all components there, like all the layers; `forward()`: we define the calculation flow in this method.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S_Dns7efEnWp",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "class linearRegression(torch.nn.Module):\n",
        "    def __init__(self, inputSize, outputSize):\n",
        "        super(linearRegression, self).__init__()\n",
        "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "msF19cisE9rw"
      },
      "source": [
        "Define inputs and outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "luCvIzM_E6cI",
        "colab": {}
      },
      "source": [
        "# our input and output are just a scalar, so the dimension should be 1 for both.\n",
        "inputDim = 1   \n",
        "outputDim = 1   \n",
        "learningRate = 0.01 \n",
        "epochs = 100\n",
        "\n",
        "# our model instance\n",
        "model = linearRegression(inputDim, outputDim)\n",
        "\n",
        "\n",
        "##### For GPU #######\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cOA9kNdIFHMH"
      },
      "source": [
        "\n",
        "## Loss function and optimizer\n",
        "\n",
        "Now it is time to define a loss function and an optimizer.  Think about what loss functions to use? And which optimizer? Fill in the code.\n",
        "\n",
        "References:\n",
        "\n",
        "https://pytorch.org/docs/stable/optim.html\n",
        "https://pytorch.org/docs/stable/nn.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NBV9bC0kFOc5",
        "colab": {}
      },
      "source": [
        "## TODO: fill in the loss function and optimizer\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "criterion = nn.MSELoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=learningRate)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=learningRate)\n",
        "optimizer = optim.SGD(model.parameters(), lr=learningRate, momentum=0.9)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WZz60Nk4FXK1"
      },
      "source": [
        "Start training now..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qKcorCJwFSOy",
        "outputId": "32a465a3-0d3e-4cdc-d851-47df547a3646",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    # Converting inputs and labels to Variable\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
        "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
        "    else:\n",
        "        inputs = Variable(torch.from_numpy(x_train))\n",
        "        labels = Variable(torch.from_numpy(y_train))\n",
        "\n",
        "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # get output from the model, given the inputs\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # get loss for the predicted output\n",
        "    loss = criterion(outputs, labels)\n",
        "    print(loss)\n",
        "    # get gradients w.r.t to parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(45.0235, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 0, loss 45.02349853515625\n",
            "tensor(41.2534, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 1, loss 41.25338363647461\n",
            "tensor(34.7014, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 2, loss 34.70137405395508\n",
            "tensor(26.6590, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 3, loss 26.659048080444336\n",
            "tensor(18.4480, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 4, loss 18.448022842407227\n",
            "tensor(11.1939, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 5, loss 11.193936347961426\n",
            "tensor(5.6759, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 6, loss 5.675943374633789\n",
            "tensor(2.2620, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 7, loss 2.262006998062134\n",
            "tensor(0.9244, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 8, loss 0.9243513345718384\n",
            "tensor(1.3173, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 9, loss 1.3172560930252075\n",
            "tensor(2.8925, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 10, loss 2.892535448074341\n",
            "tensor(5.0265, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 11, loss 5.026544570922852\n",
            "tensor(7.1356, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 12, loss 7.135609149932861\n",
            "tensor(8.7631, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 13, loss 8.763111114501953\n",
            "tensor(9.6292, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 14, loss 9.62923812866211\n",
            "tensor(9.6424, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 15, loss 9.64237117767334\n",
            "tensor(8.8774, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 16, loss 8.877420425415039\n",
            "tensor(7.5310, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 17, loss 7.531042098999023\n",
            "tensor(5.8654, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 18, loss 5.86539363861084\n",
            "tensor(4.1519, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 19, loss 4.151899814605713\n",
            "tensor(2.6242, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 20, loss 2.624241352081299\n",
            "tensor(1.4465, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 21, loss 1.4465464353561401\n",
            "tensor(0.6991, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 22, loss 0.6991111636161804\n",
            "tensor(0.3807, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 23, loss 0.38066133856773376\n",
            "tensor(0.4236, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 24, loss 0.4236450493335724\n",
            "tensor(0.7176, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 25, loss 0.7175538539886475\n",
            "tensor(1.1349, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 26, loss 1.1349096298217773\n",
            "tensor(1.5551, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 27, loss 1.5551384687423706\n",
            "tensor(1.8828, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 28, loss 1.8827991485595703\n",
            "tensor(2.0582, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 29, loss 2.058248996734619\n",
            "tensor(2.0604, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 30, loss 2.060413360595703\n",
            "tensor(1.9027, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 31, loss 1.9027189016342163\n",
            "tensor(1.6241, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 32, loss 1.6241389513015747\n",
            "tensor(1.2778, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 33, loss 1.2777633666992188\n",
            "tensor(0.9192, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 34, loss 0.9192333817481995\n",
            "tensor(0.5970, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 35, loss 0.5969685912132263\n",
            "tensor(0.3455, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 36, loss 0.34545257687568665\n",
            "tensor(0.1821, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 37, loss 0.18209411203861237\n",
            "tensor(0.1075, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 38, loss 0.10750669240951538\n",
            "tensor(0.1085, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 39, loss 0.10850383341312408\n",
            "tensor(0.1628, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 40, loss 0.1628025770187378\n",
            "tensor(0.2443, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 41, loss 0.2443375289440155\n",
            "tensor(0.3282, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 42, loss 0.32819411158561707\n",
            "tensor(0.3944, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 43, loss 0.3944217562675476\n",
            "tensor(0.4303, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 44, loss 0.43031278252601624\n",
            "tensor(0.4311, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 45, loss 0.43106338381767273\n",
            "tensor(0.3990, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 46, loss 0.3990125060081482\n",
            "tensor(0.3419, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 47, loss 0.3418585956096649\n",
            "tensor(0.2703, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 48, loss 0.27033549547195435\n",
            "tensor(0.1958, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 49, loss 0.19583556056022644\n",
            "tensor(0.1284, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 50, loss 0.12837988138198853\n",
            "tensor(0.0752, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 51, loss 0.07520141452550888\n",
            "tensor(0.0401, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 52, loss 0.040060266852378845\n",
            "tensor(0.0233, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 53, loss 0.023260358721017838\n",
            "tensor(0.0222, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 54, loss 0.022235188633203506\n",
            "tensor(0.0325, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 55, loss 0.032495252788066864\n",
            "tensor(0.0487, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 56, loss 0.04871385544538498\n",
            "tensor(0.0657, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 57, loss 0.06574588268995285\n",
            "tensor(0.0794, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 58, loss 0.07942432165145874\n",
            "tensor(0.0870, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 59, loss 0.08704691380262375\n",
            "tensor(0.0875, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 60, loss 0.08752969652414322\n",
            "tensor(0.0813, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 61, loss 0.08126675337553024\n",
            "tensor(0.0698, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 62, loss 0.06977596133947372\n",
            "tensor(0.0552, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 63, loss 0.05522601306438446\n",
            "tensor(0.0399, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 64, loss 0.0399487279355526\n",
            "tensor(0.0260, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 65, loss 0.02601795084774494\n",
            "tensor(0.0150, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 66, loss 0.014950252138078213\n",
            "tensor(0.0076, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 67, loss 0.007556272204965353\n",
            "tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 68, loss 0.003936143592000008\n",
            "tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 69, loss 0.003593272529542446\n",
            "tensor(0.0056, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 70, loss 0.005625065416097641\n",
            "tensor(0.0089, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 71, loss 0.00894451979547739\n",
            "tensor(0.0125, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 72, loss 0.012490598484873772\n",
            "tensor(0.0154, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 73, loss 0.015394489280879498\n",
            "tensor(0.0171, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 74, loss 0.017083067446947098\n",
            "tensor(0.0173, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 75, loss 0.017314566299319267\n",
            "tensor(0.0162, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 76, loss 0.016152823343873024\n",
            "tensor(0.0139, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 77, loss 0.013896717689931393\n",
            "tensor(0.0110, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 78, loss 0.010984780266880989\n",
            "tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 79, loss 0.007894479669630527\n",
            "tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 80, loss 0.005055072717368603\n",
            "tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 81, loss 0.0027848484460264444\n",
            "tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 82, loss 0.0012585282092913985\n",
            "tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 83, loss 0.0005048222956247628\n",
            "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 84, loss 0.00042845733696594834\n",
            "tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 85, loss 0.0008484742138534784\n",
            "tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 86, loss 0.0015435069799423218\n",
            "tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 87, loss 0.00229517905972898\n",
            "tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 88, loss 0.002922705840319395\n",
            "tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 89, loss 0.0033050011843442917\n",
            "tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 90, loss 0.0033886136952787638\n",
            "tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 91, loss 0.0031833460088819265\n",
            "tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 92, loss 0.0027480628341436386\n",
            "tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 93, loss 0.0021714370232075453\n",
            "tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 94, loss 0.0015513238031417131\n",
            "tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 95, loss 0.0009766612201929092\n",
            "tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 96, loss 0.0005142423906363547\n",
            "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 97, loss 0.00020172694348730147\n",
            "tensor(4.6786e-05, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 98, loss 4.678631012211554e-05\n",
            "tensor(3.1316e-05, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 99, loss 3.13156888296362e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ii14nmnmFiNf"
      },
      "source": [
        "Print..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ds2XdCD7Fjim",
        "outputId": "1bdb0cb7-57c6-4cef-954a-f9d33b60abc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "with torch.no_grad(): # we don't need gradients in the testing phase\n",
        "    if torch.cuda.is_available():\n",
        "        predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()\n",
        "    else:\n",
        "        predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
        "    # print(predicted)\n",
        "\n",
        "plt.clf()\n",
        "plt.plot(x_train, y_train, 'go', label='True', alpha=0.5)\n",
        "plt.plot(x_train, predicted, 'ro', label='Predictions', alpha=0.5)\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFb2P1wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JW0A6MeBrN6t"
      },
      "source": [
        "## Improving the model\n",
        "\n",
        "Now it is your time to improve the model: how can the prediction be better adapted to the true values? Does two linears help? Try it out!\n",
        "(Add ref)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "163EqEL-IgsX",
        "colab": {}
      },
      "source": [
        "## TODO: make your regression class\n",
        "class myRegression(torch.nn.Module):\n",
        "    def __init__(self,inputSize, outputSize):\n",
        "        super(myRegression, self).__init__()\n",
        "        #...\n",
        "        self.linear1 = torch.nn.Linear(inputSize, inputSize)\n",
        "        self.linear2 = torch.nn.Linear(inputSize, outputSize)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #...\n",
        "        out1 = self.linear1(x)\n",
        "        out = self.linear2(out1)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TAjft2riJjbP",
        "colab": {}
      },
      "source": [
        "# apply your regression class\n",
        "model = myRegression(inputDim, outputDim)\n",
        "criterion = nn.MSELoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=learningRate)\n",
        "optimizer = optim.SGD(model.parameters(), lr=learningRate, momentum=0.9)\n",
        "\n",
        "##### For GPU #######\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GQ_f-RGFJoAv",
        "outputId": "ae9e6a2e-ea90-4a9d-f2d0-066ac4b7c600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# start training\n",
        "for epoch in range(epochs):\n",
        "    # Converting inputs and labels to Variable\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
        "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
        "    else:\n",
        "        inputs = Variable(torch.from_numpy(x_train))\n",
        "        labels = Variable(torch.from_numpy(y_train))\n",
        "\n",
        "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # get output from the model, given the inputs\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # get loss for the predicted output\n",
        "    loss = criterion(outputs, labels)\n",
        "    print(loss)\n",
        "    # get gradients w.r.t to parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(60.0251, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 0, loss 60.02509689331055\n",
            "tensor(57.0901, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 1, loss 57.09014892578125\n",
            "tensor(51.9598, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 2, loss 51.95983123779297\n",
            "tensor(45.0429, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 3, loss 45.04291915893555\n",
            "tensor(35.9589, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 4, loss 35.958919525146484\n",
            "tensor(23.9849, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 5, loss 23.984949111938477\n",
            "tensor(10.2066, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 6, loss 10.20655632019043\n",
            "tensor(1.8700, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 7, loss 1.8700045347213745\n",
            "tensor(10.0554, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 8, loss 10.055380821228027\n",
            "tensor(23.6452, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 9, loss 23.645198822021484\n",
            "tensor(16.7780, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 10, loss 16.778043746948242\n",
            "tensor(4.5259, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 11, loss 4.525904655456543\n",
            "tensor(3.2623, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 12, loss 3.262312889099121\n",
            "tensor(7.4893, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 13, loss 7.489299774169922\n",
            "tensor(10.9698, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 14, loss 10.969758033752441\n",
            "tensor(12.0572, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 15, loss 12.057208061218262\n",
            "tensor(11.2110, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 16, loss 11.21102237701416\n",
            "tensor(9.2987, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 17, loss 9.29874038696289\n",
            "tensor(6.9966, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 18, loss 6.99660587310791\n",
            "tensor(4.7541, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 19, loss 4.754130840301514\n",
            "tensor(2.9732, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 20, loss 2.9731647968292236\n",
            "tensor(2.0783, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 21, loss 2.0782697200775146\n",
            "tensor(2.2107, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 22, loss 2.210742712020874\n",
            "tensor(2.7377, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 23, loss 2.7376697063446045\n",
            "tensor(2.5365, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 24, loss 2.5364880561828613\n",
            "tensor(1.3306, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 25, loss 1.3306186199188232\n",
            "tensor(0.2141, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 26, loss 0.21406522393226624\n",
            "tensor(0.2935, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 27, loss 0.2935273349285126\n",
            "tensor(1.2725, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 28, loss 1.2724987268447876\n",
            "tensor(1.7749, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 29, loss 1.7749351263046265\n",
            "tensor(1.1338, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 30, loss 1.133782982826233\n",
            "tensor(0.3673, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 31, loss 0.36733096837997437\n",
            "tensor(0.6590, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 32, loss 0.659001350402832\n",
            "tensor(1.4582, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 33, loss 1.4581868648529053\n",
            "tensor(1.4152, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 34, loss 1.4151664972305298\n",
            "tensor(0.6538, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 35, loss 0.6538430452346802\n",
            "tensor(0.3144, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 36, loss 0.31439313292503357\n",
            "tensor(0.5997, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 37, loss 0.599664568901062\n",
            "tensor(0.7757, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 38, loss 0.7757064700126648\n",
            "tensor(0.4728, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 39, loss 0.47283247113227844\n",
            "tensor(0.0787, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 40, loss 0.07874379307031631\n",
            "tensor(0.0319, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 41, loss 0.03186379745602608\n",
            "tensor(0.2591, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 42, loss 0.25912147760391235\n",
            "tensor(0.3952, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 43, loss 0.3952081501483917\n",
            "tensor(0.2947, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 44, loss 0.29473310708999634\n",
            "tensor(0.1384, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 45, loss 0.13836202025413513\n",
            "tensor(0.1252, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 46, loss 0.1251707375049591\n",
            "tensor(0.2401, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 47, loss 0.24007350206375122\n",
            "tensor(0.3258, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 48, loss 0.3257690370082855\n",
            "tensor(0.2835, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 49, loss 0.28350892663002014\n",
            "tensor(0.1644, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 50, loss 0.1643514335155487\n",
            "tensor(0.0877, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 51, loss 0.0876905769109726\n",
            "tensor(0.1025, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 52, loss 0.10249696671962738\n",
            "tensor(0.1453, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 53, loss 0.14529363811016083\n",
            "tensor(0.1313, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 54, loss 0.13125869631767273\n",
            "tensor(0.0599, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 55, loss 0.059896036982536316\n",
            "tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 56, loss 0.005107422824949026\n",
            "tensor(0.0152, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 57, loss 0.015200530178844929\n",
            "tensor(0.0584, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 58, loss 0.05844658240675926\n",
            "tensor(0.0735, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 59, loss 0.07348348200321198\n",
            "tensor(0.0470, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 60, loss 0.04704919829964638\n",
            "tensor(0.0206, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 61, loss 0.020623216405510902\n",
            "tensor(0.0286, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 62, loss 0.028619548305869102\n",
            "tensor(0.0553, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 63, loss 0.0552925206720829\n",
            "tensor(0.0623, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 64, loss 0.06232370063662529\n",
            "tensor(0.0410, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 65, loss 0.04098677635192871\n",
            "tensor(0.0177, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 66, loss 0.017695846036076546\n",
            "tensor(0.0147, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 67, loss 0.01471574418246746\n",
            "tensor(0.0247, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 68, loss 0.02470567636191845\n",
            "tensor(0.0269, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 69, loss 0.026851706206798553\n",
            "tensor(0.0151, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 70, loss 0.015094521455466747\n",
            "tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 71, loss 0.002292506629601121\n",
            "tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 72, loss 0.0011862332466989756\n",
            "tensor(0.0095, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 73, loss 0.009463046677410603\n",
            "tensor(0.0153, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 74, loss 0.015284880064427853\n",
            "tensor(0.0126, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 75, loss 0.01260099746286869\n",
            "tensor(0.0066, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 76, loss 0.006577191408723593\n",
            "tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 77, loss 0.005058406386524439\n",
            "tensor(0.0089, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 78, loss 0.008877089247107506\n",
            "tensor(0.0122, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 79, loss 0.012248901650309563\n",
            "tensor(0.0106, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 80, loss 0.010639416985213757\n",
            "tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 81, loss 0.00573194632306695\n",
            "tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 82, loss 0.0024767036084085703\n",
            "tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 83, loss 0.002984657883644104\n",
            "tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 84, loss 0.0047369650565087795\n",
            "tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 85, loss 0.004419910255819559\n",
            "tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 86, loss 0.001982757356017828\n",
            "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 87, loss 0.00012920268636662513\n",
            "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 88, loss 0.0006323425914160907\n",
            "tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 89, loss 0.002376519376412034\n",
            "tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 90, loss 0.003123711096122861\n",
            "tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 91, loss 0.002266450086608529\n",
            "tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 92, loss 0.0011742673814296722\n",
            "tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 93, loss 0.0011762004578486085\n",
            "tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 94, loss 0.0019839522428810596\n",
            "tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 95, loss 0.002332500647753477\n",
            "tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 96, loss 0.0016697683604434133\n",
            "tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 97, loss 0.0006962613551877439\n",
            "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 98, loss 0.0003320591349620372\n",
            "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 99, loss 0.0006272256723605096\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ROWeKmTBJwsP",
        "outputId": "0845c4b8-9603-4be5-d129-f96f13e7773f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "with torch.no_grad(): # we don't need gradients in the testing phase\n",
        "    if torch.cuda.is_available():\n",
        "        predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()\n",
        "    else:\n",
        "        predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
        "    print(predicted)\n",
        "\n",
        "plt.clf()\n",
        "plt.plot(x_train, y_train, 'go', label='True data', alpha=0.5)\n",
        "plt.plot(x_train, predicted, 'ro', label='Predictions', alpha=0.5)\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.1738025]\n",
            " [ 1.3777344]\n",
            " [ 1.5816662]\n",
            " [ 1.7855984]\n",
            " [ 1.9895302]\n",
            " [ 2.1934621]\n",
            " [ 2.3973942]\n",
            " [ 2.601326 ]\n",
            " [ 2.8052578]\n",
            " [ 3.0091896]\n",
            " [ 3.2131217]\n",
            " [ 3.4170535]\n",
            " [ 3.6209855]\n",
            " [ 3.8249173]\n",
            " [ 4.0288496]\n",
            " [ 4.2327814]\n",
            " [ 4.436713 ]\n",
            " [ 4.640645 ]\n",
            " [ 4.844577 ]\n",
            " [ 5.048509 ]\n",
            " [ 5.252441 ]\n",
            " [ 5.4563727]\n",
            " [ 5.6603045]\n",
            " [ 5.864237 ]\n",
            " [ 6.0681686]\n",
            " [ 6.2721004]\n",
            " [ 6.4760323]\n",
            " [ 6.679964 ]\n",
            " [ 6.8838964]\n",
            " [ 7.087828 ]\n",
            " [ 7.29176  ]\n",
            " [ 7.495692 ]\n",
            " [ 7.6996236]\n",
            " [ 7.9035554]\n",
            " [ 8.107488 ]\n",
            " [ 8.3114195]\n",
            " [ 8.515351 ]\n",
            " [ 8.719283 ]\n",
            " [ 8.923215 ]\n",
            " [ 9.127147 ]\n",
            " [ 9.3310795]\n",
            " [ 9.535011 ]\n",
            " [ 9.738943 ]\n",
            " [ 9.942875 ]\n",
            " [10.146807 ]\n",
            " [10.350739 ]\n",
            " [10.554671 ]\n",
            " [10.758603 ]\n",
            " [10.962535 ]\n",
            " [11.166467 ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ea7XpQP0XROO"
      },
      "source": [
        "# Part A2\n",
        "Once you have sshed into the Zoo cluster, copy the homework files to your hidden directory under Homework2 folder.\n",
        "`cp -r /home/classes/cs477/assignments/2020-Homework2 ~/hidden/<YOUR_PIN>`\n",
        "\n",
        "Apply the virtualenv using:\n",
        "`source /home/classes/cs477/venvs/hw2/bin/activate`\n",
        "\n",
        "In this part you will fill the code in `PartA2.py`.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W4NhpKwfrEEZ"
      },
      "source": [
        "## Nave Bayes\n",
        "Time estimate to run correct implementation: 5 minutes\n",
        "In this part of the assignment, youll be training a Nave Bayes Classifier on the IMDb Movie Review Database. Youll be using some of the features that come built into the sklearn library to quickly get a Multinomial Nave Bayes Classifier running. \n",
        "\n",
        "1. To process the movie reviews, well want to convert them into a matrix of word counts for each possible word. We recommend that you check out the CountVectorizer class in sklearn. You can use the nltk word tokenizer as well. Your code for this function should be very brief!\n",
        "2. Well train and test a Nave Bayes Classifier on our dataset. Using sklearns MultinomialNB() class, fit a classifier to the reviews and labels of the training set (x_train and y_train). Then return a set of predictions for the test set of reviews (x_test)\n",
        "\n",
        "The result we got with a correct implementation was around:\n",
        "Accuracy: 0.660000\n",
        "(Around this number, may be different). \n",
        "\n",
        "Note that it may take a few minutes to run the script. \n",
        "\n",
        "\n",
        "Fill the code in `PartA2.py`.  \n",
        "\n",
        "*Reference*: https://scikit-learn.org/stable/datasets/index.html#datasets\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OOm0KSXarkqu"
      },
      "source": [
        "### **Submission**\n",
        "You have to submit the code (.ipynb) file, and answer the questions in a Text cell (click `+Text` button to do so), make sure that your answer is in the LAST Text cell. Also make sure that your .ipynb contains all the outputs, i.e., the plots.  You can also save your answer in a `README.txt` file and submit. \n",
        "\n",
        "\n",
        "**Questions:**\n",
        "1. What improvements did you try in the `myRegression` class?  Describe the structure. And do you think it improved? \n",
        "2. Do you think only using `nn.Linear` layers can learn this function: $y=sin(x)$? Why or why not?\n",
        "\n",
        "\n",
        "## Submission\n",
        "\n",
        "Now that you have completed the assignment, follow the steps below to submit your aissgnment:\n",
        "1. Click __Runtime__  > __Run all__ to generate the output for all cells in the notebook.\n",
        "2. Save the notebook ( __File__ >  __Save__) with the output from all the cells in the notebook by click __File__ > __Download .ipynb__.\n",
        "3. Put the .ipynb file and `README.txt` (if you have one) under your hidden directory on the Zoo server `~/hidden/<YOUR_PIN>/Homework2/`.\n",
        "5. As a final step, run a script that will set up the permissions to your homework files, so we can access and run your code to grade it. Make sure the command be;pw runs without errors, and do not make any changes or run the code again. If you do run the code again or make any changes, you need to run the permissions script again. Submissions without the correct permissions may incur some grading penalty.\n",
        "`/home/classes/cs477/bash_files/hw2_set_permissions.sh <YOUR_PIN>`. **Note** that your `PartA2.py` should also be included in your folder."
      ]
    }
  ]
}