{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PartC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2BGK1XLEf8r",
        "colab_type": "text"
      },
      "source": [
        "# Transformers and BERT\n",
        "In this part, we will learn how to apply pre-trained BERT model to improve text classification. Bidirectional Encoder Representations from Transformers (BERT) is a technique for NLP (Natural Language Processing) pre-training developed by Google. BERT was created and published in 2018 by Jacob Devlin and his colleagues from Google. Google is leveraging BERT to better understand user searches. (From WIKI)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Read more: http://jalammar.github.io/illustrated-transformer/\n",
        "\n",
        "BERT paper: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\n",
        "\n",
        "Understanding BERT: https://towardsdatascience.com/understanding-bert-is-it-a-game-changer-in-nlp-7cca943cf3ad\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhsCy3dQEpmu",
        "colab_type": "text"
      },
      "source": [
        "## Preparing Dataset\n",
        "Set random seed\n",
        "\n",
        "Make sure that you are using Python3 and a GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7keSeDS2EZyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "SEED = 1001\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu_jD_f1VQyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A way to enlarge RAM: https://towardsdatascience.com/upgrade-your-memory-on-google-colab-for-free-1b8b18e8791d\n",
        "# d=[]\n",
        "# while(1):\n",
        "#   d.append('1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTwlt9C8ughd",
        "colab_type": "text"
      },
      "source": [
        "We use an existing library called `transformers` to import BERT models.  Now let's install it first. \n",
        "\n",
        "Read more in the repo: https://github.com/huggingface/transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cf_NmmMEzUR",
        "colab_type": "code",
        "outputId": "96dc551a-7165-417f-8a13-79e431cfdb1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "# make sure that transformers library is installed\n",
        "! pip install transformers\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXTHansbvA4A",
        "colab_type": "text"
      },
      "source": [
        "We now import the tokenizer, this is to tokenize sentences. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JueLVzpHEvOA",
        "colab_type": "code",
        "outputId": "31880349-9a51-47f0-cfa7-d44e8f7ec747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "# let's use a pre-trained version ('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
        "\n",
        "print(max_input_length)\n",
        "\n",
        "len(tokenizer.vocab)\n",
        "\n",
        "# tokenize a sentence: you will see the tokenizer \"cleans\" the sentence as well.\n",
        "tokens = tokenizer.tokenize('Hello WORLD how ARE yoU?')\n",
        "\n",
        "print(tokens)\n",
        "\n",
        "# There will be a warning, but just leave it"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "512\n",
            "['hello', 'world', 'how', 'are', 'you', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX9jWiW9vQC2",
        "colab_type": "text"
      },
      "source": [
        "Now we convert the tokens into IDs.\n",
        "\n",
        "And we list the IDs, and some spcial tokens: `<CLS>` means classification token; `<SEP>` means a separator between two sentences, and so on..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HBjW5VAE9nn",
        "colab_type": "code",
        "outputId": "3c4db713-c9e8-4897-a711-771b58bd205e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(indexes)\n",
        "\n",
        "init_token = tokenizer.cls_token\n",
        "eos_token = tokenizer.sep_token\n",
        "pad_token = tokenizer.pad_token\n",
        "unk_token = tokenizer.unk_token\n",
        "\n",
        "print(init_token, eos_token, pad_token, unk_token)\n",
        "\n",
        "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
        "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
        "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
        "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
        "\n",
        "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[7592, 2088, 2129, 2024, 2017, 1029]\n",
            "[CLS] [SEP] [PAD] [UNK]\n",
            "101 102 0 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJbMuqdmFNOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_and_cut(sentence):\n",
        "    tokens = tokenizer.tokenize(sentence) \n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJCwGREyvx0c",
        "colab_type": "text"
      },
      "source": [
        "Prepare TEXT and LABEL ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2FwmclZFQH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext import data\n",
        "\n",
        "TEXT = data.Field(batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  tokenize = tokenize_and_cut,\n",
        "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "                  init_token = init_token_idx,\n",
        "                  eos_token = eos_token_idx,\n",
        "                  pad_token = pad_token_idx,\n",
        "                  unk_token = unk_token_idx)\n",
        "\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfiCUKTOVKUC",
        "colab_type": "code",
        "outputId": "67f8f060-d71e-4648-e079-45479ddb4416",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        " # follow the steps to authorize colab to get access to your google drive data\n",
        " from google.colab import drive\n",
        " drive.mount('/content/gdrive')\n",
        " # set up the path\n",
        "ROOT_DIR = \" gdrive/My\\ Drive/Colab\\ Notebooks/nlp_hw2/\"\n",
        "DATA_DIR = ROOT_DIR+'IMDB.gz'\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ler852YiFSO3",
        "colab_type": "code",
        "outputId": "d65da5a1-5633-43ec-db32-db576e26a2e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "## TODO: Let's use the IMDB data, and split into training and testing (this may take a few minutes)\n",
        "from torchtext import datasets\n",
        "\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "train_data, valid_data = train_data.split(random_state=random.seed(SEED))\n",
        "# all_data = datasets.IMDB(DATA_DIR,TEXT, LABEL)\n",
        "# train_data, test_data = all_data.splits(TEXT, LABEL)\n",
        "\n",
        "# train_data, valid_data = ...\n",
        "\n",
        "\n",
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data)}\")\n",
        "print(f\"Number of testing examples: {len(test_data)}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 17500\n",
            "Number of validation examples: 7500\n",
            "Number of testing examples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOE8PJcQF9-r",
        "colab_type": "code",
        "outputId": "88a7131d-1f70-4beb-bb24-a5ced0277e16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[6])['text'])\n",
        "\n",
        "print(tokens)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['a', 'typical', 'clause', '##n', 'film', ',', 'but', 'then', 'again', 'not', 'typical', '.', 'clause', '##n', 'writes', ',', 'directs', 'and', 'play', 'one', 'of', 'the', 'leading', 'roles', '.', 'this', 'is', 'really', 'a', 'great', 'film', 'about', 'normal', 'people', 'living', 'normal', 'lives', 'trying', 'to', 'make', 'the', 'best', 'of', 'it', '.', 'the', '4', 'primary', 'actors', 'were', 'fantastic', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'fritz', 'helmut', 'was', 'convincing', '.', 'you', 'believe', 'that', 'he', 'really', 'is', 'sick', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'son', '##ja', 'richter', 'plays', 'a', 'nurse', 'that', 'really', 'is', 'an', 'actor', ',', 'but', 'it', 'turns', 'out', 'that', 'she', 'is', 'the', 'best', 'nurse', 'to', 'take', 'care', 'of', 'the', 'old', 'man', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'everybody', 'has', 'problems', 'and', 'those', 'who', 'nobody', 'believes', 'in', 'ends', 'up', 'being', 'happy', '.', 'but', 'nothing', 'good', 'comes', 'easy', ',', 'they', 'have', 'to', 'fight', 'to', 'win', 'their', 'life', 'and', 'love', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzeWiB_8F_fA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build vocab\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwPwq8fNGMoe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE, \n",
        "    device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IiTsTu7GQZ1",
        "colab_type": "text"
      },
      "source": [
        "## Model building\n",
        "\n",
        "Import the `BertModel`, and we load from the pre-trained model by giving the path. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZpl05CbGSeX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertModel\n",
        "# It will download the pre-trained model\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YfnzOCAZI6T",
        "colab_type": "text"
      },
      "source": [
        "## Applying BERT to classification\n",
        "\n",
        "It is possible to use the BERT model directly, however, the free GPU is not large enough to load the whole model; \n",
        "So let's try to use the pre-trained embedding layer. Then we train our own RNN layer on top of it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHTCN8nnGYMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MyBERTwithRNN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_dim,\n",
        "                 output_dim,\n",
        "                 n_layers,\n",
        "                 bidirectional,\n",
        "                 dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.bert = bert\n",
        "        \n",
        "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "        \n",
        "        self.rnn = nn.GRU(embedding_dim,\n",
        "                          hidden_dim,\n",
        "                          num_layers = n_layers,\n",
        "                          bidirectional = bidirectional,\n",
        "                          batch_first = True,\n",
        "                          dropout = 0 if n_layers < 2 else dropout)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        #text = [batch size, sent len]\n",
        "                \n",
        "        with torch.no_grad():\n",
        "            embedded = self.bert(text)[0]\n",
        "                \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        _, hidden = self.rnn(embedded)\n",
        "        \n",
        "        #hidden = [n layers * n directions, batch size, emb dim]\n",
        "        \n",
        "        if self.rnn.bidirectional:\n",
        "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "        else:\n",
        "            hidden = self.dropout(hidden[-1,:,:])\n",
        "                \n",
        "        #hidden = [batch size, hid dim]\n",
        "        \n",
        "        output = self.out(hidden)\n",
        "        \n",
        "        #output = [batch size, out dim]\n",
        "        \n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpTj_vfDGig5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## creat model instance\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.25\n",
        "\n",
        "model = MyBERTwithRNN(bert,\n",
        "                         HIDDEN_DIM,\n",
        "                         OUTPUT_DIM,\n",
        "                         N_LAYERS,\n",
        "                         BIDIRECTIONAL,\n",
        "                         DROPOUT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtD_O5xwGqVA",
        "colab_type": "code",
        "outputId": "1811bb83-7b94-4f86-c0b2-342eade45682",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    ## fill here:\n",
        "    param_number = 0\n",
        "    param_number = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "    return param_number\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 112,241,409 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGHTehzQGu3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's fix the bert embeddings\n",
        "for name, param in model.named_parameters():                \n",
        "    if name.startswith('bert'):\n",
        "        param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-So8zd6HwXZ",
        "colab_type": "text"
      },
      "source": [
        "## Model Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SS6DfSHN-rB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr_yTNfNHy4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## TODO: define the accuracy function (Hint: similar to the same function from PartB)\n",
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() \n",
        "    acc = correct.sum()/len(correct)\n",
        "    return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJetyYsjN2Wm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## TODO: finish the training function\n",
        "## model is our model; iterator contains data in batches; criterion is to calculate the loss function\n",
        "## we want to return the average epoch loss and epoch accuracy.  (Hint: use binary_accuracy() )\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        # ...\n",
        "        optimizer.zero_grad()\n",
        "        batch_pred = model(batch.text).squeeze()\n",
        "        loss = criterion(batch_pred, batch.label)\n",
        "        acc = binary_accuracy(batch_pred, batch.label)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3Rb2_kUNoaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## TODO: finish the evaluation function\n",
        "## model is our model; iterator contains data in batches; criterion is to calculate the loss function\n",
        "## we want to return the average epoch loss and epoch accuracy.   (Hint: use binary_accuracy() )\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "            # ...\n",
        "            batch_pred = model(batch.text).squeeze()\n",
        "            loss = criterion(batch_pred, batch.label)\n",
        "            acc = binary_accuracy(batch_pred, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKX3rfEQRwPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "# a helper function to see how much time needed\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-KFFkv6Nurd",
        "colab_type": "code",
        "outputId": "fc4e044c-f589-4b1e-c70a-10616eb525f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Start training.\n",
        "# Note that it will take ~17 minutes for one epoch.\n",
        "# The output will be: Epoch: 01 | Epoch Time: 17m 36s...\n",
        "# Validate accuracy is higher than 85% in the first epoch, higher than 90% in the second epoch.\n",
        "\n",
        "N_EPOCHS = 2\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "        \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 6m 46s\n",
            "\tTrain Loss: 0.480 | Train Acc: 75.87%\n",
            "\t Val. Loss: 0.267 |  Val. Acc: 89.21%\n",
            "Epoch: 02 | Epoch Time: 6m 46s\n",
            "\tTrain Loss: 0.268 | Train Acc: 89.05%\n",
            "\t Val. Loss: 0.242 |  Val. Acc: 90.37%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZeSgxe5OKlC",
        "colab_type": "code",
        "outputId": "d2e8b70b-fada-4a69-aa8e-80eef46d75e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load the best model and evaluate; this may take about 5-10 mins; the Test Accuracy is higher than 90%\n",
        "model.load_state_dict(torch.load('best_model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.246 | Test Acc: 90.27%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHRaEwbWOWrY",
        "colab_type": "text"
      },
      "source": [
        "## Inference on your own sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZVbz-ohOd0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Each sentence when converting into the index, should have [CLS] tag at the beginning and [SEP] tag in the end. \n",
        "def my_predict(model, tokenizer, sentence):\n",
        "    model.eval()\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(0)\n",
        "    prediction = torch.sigmoid(model(tensor))\n",
        "    return prediction.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwtE71M_Ohwe",
        "colab_type": "code",
        "outputId": "698bead6-2c8d-4a15-fc79-e0ac7a30f699",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# the score should be close to 0\n",
        "my_predict(model, tokenizer, \"This film is terrible\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09066072851419449"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bYjd5AVOnPY",
        "colab_type": "code",
        "outputId": "62e0c341-ceec-495c-92b7-7edcb22779cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# the score should be close to 1\n",
        "my_predict(model, tokenizer, \"I like it !\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9777471423149109"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDJ_9Eh9XYq2",
        "colab_type": "text"
      },
      "source": [
        "**Question**: how do you compare the bert model with PartB? (Hint: training time, accuracy, etc.) Please answer in the next Text cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD9S-636Xpbc",
        "colab_type": "text"
      },
      "source": [
        "**Answer**: Bert can achieve better score than the networks![alt text](https://) in Part B in term of accuracy. However, Bert require more training time compared with networks in Part B."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgPH1Z_9wWID",
        "colab_type": "text"
      },
      "source": [
        "## Submission\n",
        "\n",
        "Now that you have completed the assignment, follow the steps below to submit your aissgnment:\n",
        "1. Click __Runtime__  > __Run all__ to generate the output for all cells in the notebook. \n",
        "2. Save the notebook (__File__  > __Save__) with the output from all the cells in the notebook by click __File__ > __Download .ipynb__.\n",
        "3. **Keep the output cells** , and answers to the question in the Text cell. \n",
        "4. Put the .ipynb file under your hidden directory on the Zoo server `~/hidden/<YOUR_PIN>/Homework2/`.\n",
        "5. As a final step, run a script that will set up the permissions to your homework files, so we can access and run your code to grade it. Make sure the command be;pw runs without errors, and do not make any changes or run the code again. If you do run the code again or make any changes, you need to run the permissions script again. Submissions without the correct permissions may incur some grading penalty.\n",
        "`/home/classes/cs477/bash_files/hw2_set_permissions.sh <YOUR_PIN>`\n"
      ]
    }
  ]
}