{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PartB.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aOozaxFNG21",
        "colab_type": "code",
        "outputId": "8ac6cdcc-ffe2-4158-fc71-7c278a9883db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "# Check if a gpu is available\n",
        "!nvidia-smi\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Mar  1 02:27:52 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.48.02    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P8    18W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czxD_qGxNMec",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Classification\n",
        "\n",
        "In this assignment we will:\n",
        "1. Use Pytorch to load the IMDb movie dataset and do preprocessing;\n",
        "2. Develop a Recurrent Neural Network (RNN) Classifier for the same dataset;\n",
        "3. Convert the RNN to a bidirectional Long-Short-Term-Memory (LSTM) model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF3du_w9t8CE",
        "colab_type": "text"
      },
      "source": [
        "## 1. Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L67uXDrvNQhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "\n",
        "SEED = 12138\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Torchtext will let us to load the text and labels separately.\n",
        "TEXT = data.Field(tokenize = 'spacy')\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfizApcF8Yz1",
        "colab_type": "code",
        "outputId": "01b6152f-9858-4962-e6d3-93eec5d30678",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        " # follow the steps to authorize colab to get access to your google drive data\n",
        " from google.colab import drive\n",
        " drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdz9mWho82wc",
        "colab_type": "code",
        "outputId": "671c2509-5f24-4843-e3f8-be2a80813bb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# make sure that you can see the ipynb files and IMDB.gz\n",
        "!ls  gdrive/My\\ Drive/Colab\\ Notebooks/nlp_hw2/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IMDB.gz  PartA.ipynb  PartB.ipynb  PartC.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-5JsNlBNSX1",
        "colab_type": "text"
      },
      "source": [
        "## Data loading\n",
        "Read more: https://pytorchnlp.readthedocs.io/en/latest/_modules/index.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0D2GMZSGNRlY",
        "colab_type": "code",
        "outputId": "05c4b75f-71c8-4d81-cecf-730d220c7dd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from torchtext import datasets\n",
        "import os\n",
        "\n",
        "# set up the path\n",
        "ROOT_DIR = \" gdrive/My\\ Drive/Colab\\ Notebooks/nlp_hw2/\"\n",
        "DATA_DIR = ROOT_DIR+'IMDB.gz'\n",
        "\n",
        "# load data, this may take a while\n",
        "all_data = datasets.IMDB(DATA_DIR,TEXT, LABEL)\n",
        "train_data, test_data = all_data.splits(TEXT, LABEL)\n",
        "\n",
        "print ('Loading finished!')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GieDV9WdOatS",
        "colab_type": "code",
        "outputId": "a50f7674-98d6-4ba4-9c2e-3a1e2749ab4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 25000\n",
            "Number of testing examples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkloUWsTOzV3",
        "colab_type": "code",
        "outputId": "0dc50e5e-30e1-4fbd-8c6d-eaeef015e8c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import random\n",
        "# split into train and validation set\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED))\n",
        "\n",
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 17500\n",
            "Number of validation examples: 7500\n",
            "Number of testing examples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUtcoLdTfYlQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set vocab\n",
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_aSJxDASDVe",
        "colab_type": "text"
      },
      "source": [
        "##### Define iterator\n",
        "\n",
        "Define an iterator that batches examples of similar lengths together. \n",
        "There are other options. For more: https://torchtext.readthedocs.io/en/latest/data.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsdB0cfbO7g0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "# If there is a GPU available, we will set to use it; otherwise we will use cpu.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW3gMNJdx8yV",
        "colab_type": "text"
      },
      "source": [
        "## 2. Recurrent Neural Network\n",
        "\n",
        "This part of the assignment will involve building your own Recurrent Neural Network model for the sentiment analysis task.\n",
        "\n",
        "1. The first thing you’ll want to do is fill out the code in the initialization of the RNN class. You’ll need to define three layers: self.embedding, self.rnn, and self.fc. Use the built-in functions in torch.nn to accomplish this (remember that a fully-connected layer is also a linear layer!) and pay attention to what each dimensions each layer should have for its input and output.\n",
        "2. The next step (still in the RNN class) is to implement the forward pass. Make use of the layers you defined above to create embedded, hidden, and output vectors for a given input x."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8SCAP3ISI1v",
        "colab_type": "text"
      },
      "source": [
        "Hint to start our model:\n",
        "The RNN model should have the following structure:\n",
        "1. start by an embedding layer; shape:  (input_dim, embedding_dim)\n",
        "2. then we put the RNN layer; shape: (embedding_dim, hidden_dim)\n",
        "3. last, we add a liner layer; shape: (hidden_dim, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeouwCjNSISx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "## TODO: define the RNN class\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        ## TODO starts\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        ## TODO ends\n",
        "        \n",
        "    def forward(self, text):\n",
        "\n",
        "        ## TODO starts\n",
        "        embedded = self.embedding(text)\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        result = self.fc(hidden.squeeze(0))\n",
        "        ## TODO ends\n",
        "\n",
        "        return  result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5xkXjVAeUIc",
        "colab_type": "text"
      },
      "source": [
        "## Model Training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tudzd0gqenDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define some hyperparameters\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_xjxleFeWte",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "# apply our RNN model here\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
        "\n",
        "\n",
        "## TODO: define optmizer\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "## TODO ends"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tuj4LPQhflJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "## setup device\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLJnUrnPfpIC",
        "colab_type": "text"
      },
      "source": [
        "### Calculate accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zAau631fxCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## TODO: return the accuracy given the preditions (preds) and true values (y); acc should be a float number\n",
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "\n",
        "    r_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (r_preds == y).float() \n",
        "    acc = correct.sum()/len(correct)\n",
        "\n",
        "    return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbYNoyaOgHZs",
        "colab_type": "text"
      },
      "source": [
        "## Training function\n",
        "\n",
        "The next function is the train function. Most of the code is handled for you- you only need to get a set of predictions for the current batch and then calculate the current loss and accuracy. For the latter two calculations, make sure to use the criterion and binary_accuracy functions you are given. For calculating the batch predictions, extract the text of the current batch and run it through the model, which is passed in as a parameter.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmBvYxG0gEzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## TODO: finish the training function\n",
        "## iterator contains batches of the training data; \n",
        "## hint: use batch.text and batch.label to get access to the training data and labels\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        # TODO starts\n",
        "        optimizer.zero_grad()\n",
        "        batch_pred = model(batch.text).squeeze()\n",
        "        batch_loss = criterion(batch_pred, batch.label)\n",
        "        batch_acc = binary_accuracy(batch_pred, batch.label)\n",
        "\n",
        "        ## Back\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += batch_loss.item()\n",
        "        epoch_acc += batch_acc.item()\n",
        "\n",
        "        ## TODO ends\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJl5UQqFghRl",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation function\n",
        "\n",
        "This step is to copy and paste what you did in the training function into the evaluate function. This time, there’s no additional optimization after the predictions, loss, and accuracy are calculated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5QUfm3cgnTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## TODO: finish the evaluation function\n",
        "## iterator contains batches of the training data; \n",
        "## hint: this function is very similar to the training function\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "            \n",
        "            ## TODO starts\n",
        "            batch_pred = model(batch.text).squeeze()\n",
        "            batch_loss = criterion(batch_pred, batch.label)\n",
        "            batch_acc = binary_accuracy(batch_pred, batch.label)\n",
        "\n",
        "            epoch_loss += batch_loss.item()\n",
        "            epoch_acc += batch_acc.item()\n",
        "\n",
        "            ## TODO ends\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fORHPtVHg1jh",
        "colab_type": "text"
      },
      "source": [
        "### Start training\n",
        "It may take a few minutes in total. The validate accuracy is around 50-51%.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhp0Ng6gg1_1",
        "colab_type": "code",
        "outputId": "f276dfa8-f279-490b-aa4c-dd29624c7847",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "# let's train 5 epochs\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "      \n",
        "    # we keep track of the best model, and save it\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pt')\n",
        "    \n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.698 | Train Acc: 49.54%\n",
            "\t Val. Loss: 0.702 |  Val. Acc: 49.65%\n",
            "\tTrain Loss: 0.698 | Train Acc: 49.88%\n",
            "\t Val. Loss: 0.702 |  Val. Acc: 49.19%\n",
            "\tTrain Loss: 0.700 | Train Acc: 50.22%\n",
            "\t Val. Loss: 0.696 |  Val. Acc: 48.90%\n",
            "\tTrain Loss: 0.699 | Train Acc: 49.63%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 51.24%\n",
            "\tTrain Loss: 0.697 | Train Acc: 49.74%\n",
            "\t Val. Loss: 0.697 |  Val. Acc: 51.19%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prOfpKWAhHvy",
        "colab_type": "text"
      },
      "source": [
        "### Restore the best model and evaluate\n",
        "\n",
        "The test accuracy is around 47%\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPwMRve4hMGB",
        "colab_type": "code",
        "outputId": "da013aaf-f640-4356-da04-b7c94468d747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.load_state_dict(torch.load('best_model.pt'))\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.694 | Test Acc: 49.51%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H6wZ6MN2xuY",
        "colab_type": "text"
      },
      "source": [
        "## 3. LSTM\n",
        "This step of this assignment is to modify your RNN into a bidirectional LSTM network. We’ll see that this kind of model performs much better than our previous ones.\n",
        "\n",
        "1. You’ll be making changes to your model in the RNN Class. In the init class, for the rnn layer, use the nn.LSTM function and make sure you pass in the bidirectional argument. Also note that the fully connected layer now has to map from two hidden layer passes (forward and backward).\n",
        "2. In the forward pass, not much changes from before, besides the addition of the cell. Also note that you’ll have to concatenate the final forward hidden layer and the final backward hidden layer. If any of this is unclear, look up example of how nn.lstm works for clarification.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UtRFOSw21Du",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class RNN(nn.Module):\n",
        "    # TODO: IMPLEMENT THIS FUNCTION\n",
        "    # Initialize the three layers in the RNN, self.embedding, self.rnn, and self.fc\n",
        "    # Each one has a corresponding function in nn\n",
        "    # embedding maps from input_dim->embedding_dim\n",
        "    # rnn maps from embedding_dim->hidden_dim\n",
        "    # fc maps from hidden_dim*2->output_dim\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, bidirectional):\n",
        "        super().__init__()\n",
        "        \n",
        "        ## CHANGE THESE DEFINITIONS\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, bidirectional=bidirectional)\n",
        "        self.fc = nn.Linear(2 * hidden_dim, output_dim)\n",
        "       \n",
        "    # TODO: IMPLEMENT THIS FUNCTION\n",
        "    # x has dimensions [sentence length, batch size]\n",
        "    # embedded has dimensions [sentence length, batch size, embedding_dim]\n",
        "    # output has dimensions [sentence length, batch size, hidden_dim*2] (since bidirectional)\n",
        "    # hidden has dimensions [2, batch size, hidden_dim]\n",
        "        # cell has dimensions [2, batch_size, hidden_dim]\n",
        "    # Need to concatenate the final forward and backward hidden layers\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, (hidden, cell) = self.rnn(embedded)\n",
        "        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), 1)\n",
        "        \n",
        "        return self.fc(hidden.squeeze(0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jue9iw1K24sz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# apply our RNN model here\n",
        "BIDIRECTIONAL = True\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, BIDIRECTIONAL)\n",
        "## setup device\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01yngM1yDbuu",
        "colab_type": "text"
      },
      "source": [
        "It may take a few minutes in total. The validate accuracy is around 50%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WpHP5eH3JoX",
        "colab_type": "code",
        "outputId": "4f592f01-c75c-4636-ebd3-1521fb4ddb5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "# train again!\n",
        "best_valid_loss = float('inf')\n",
        "# let's train 5 epochs\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "      \n",
        "    # we keep track of the best model, and save it\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'best_model_LSTM.pt')\n",
        "    \n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.694 | Train Acc: 49.90%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 49.79%\n",
            "\tTrain Loss: 0.694 | Train Acc: 49.75%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 49.79%\n",
            "\tTrain Loss: 0.694 | Train Acc: 49.87%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 49.79%\n",
            "\tTrain Loss: 0.694 | Train Acc: 49.80%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 49.79%\n",
            "\tTrain Loss: 0.694 | Train Acc: 49.91%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 49.79%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvLbKSVG4UPs",
        "colab_type": "code",
        "outputId": "7d860726-9576-4105-c377-a45249b20100",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.load_state_dict(torch.load('best_model_LSTM.pt'))\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.696 | Test Acc: 50.15%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvxJYUfW4Yxk",
        "colab_type": "text"
      },
      "source": [
        "**Question**: Do you think LSTM is working better than RNN? Why or why not? How do you compare with LSTM and RNN (model complexity, etc)?\n",
        "Please answer in the next Text cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGp1HgQu4rrH",
        "colab_type": "text"
      },
      "source": [
        "**Answer**: LSTM should work better than RNN. \n",
        "\n",
        "RNN is a class of neural networks that allow previous outputs to be used as inputs while having hidden states and can use their internal state (memory) to process variable length sequences of input. So they could capture information about what has been calculated. In other words, output of each node depends on computations on previous nodes. Thus, RNN have the following issues: 1) Gradient vanishing and exploding problems. 2) Cannot consider any future input for the current state. On the other hand, LSTM is the modified version of recurrent neural networks, which makes it easier to remember past data in memory. It could fix the vanishing gradient problem of RNN by using a cell with 3 gates (input gate, output gate, and forget gate).\n",
        "\n",
        "Furthermore, I implemented the Bidirectional LSTM above. Undirectional LSTM only preserves information of the past states because the only inputs it only see are from the past. Using the bidirectional can take over the inputs in two ways, one is from the past to future and another is from future to past. In other words, it can effectively preserve information from both past and future.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkWzI3OrjMtr",
        "colab_type": "text"
      },
      "source": [
        "## Submission\n",
        "\n",
        "Now that you have completed the assignment, follow the steps below to submit your aissgnment:\n",
        "1. Click __Runtime__  > __Run all__ to generate the output for all cells in the notebook.\n",
        "2. Save the notebook with the output from all the cells in the notebook by click __File__ > __Download .ipynb__.\n",
        "3. **Keep the output cells** , and answers to the question in the Text cell. \n",
        "4. Put the .ipynb file under your hidden directory on the Zoo server `~/hidden/<YOUR_PIN>/Homework2/`.\n",
        "5. As a final step, run a script that will set up the permissions to your homework files, so we can access and run your code to grade it. Make sure the command be;pw runs without errors, and do not make any changes or run the code again. If you do run the code again or make any changes, you need to run the permissions script again. Submissions without the correct permissions may incur some grading penalty.\n",
        "`/home/classes/cs477/bash_files/hw2_set_permissions.sh <YOUR_PIN>`"
      ]
    }
  ]
}